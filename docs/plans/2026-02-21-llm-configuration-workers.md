# LLM Configuration Workers

**Date:** 2026-02-21
**Status:** Design — not yet implemented
**Context:** Architectural evolution of the driver enforcement model

---

## The Problem With Hardcoded Drivers

The current driver model requires each driver to know the exact config mutations for every directive it supports:

```go
// OpenClaw driver must know this exact key path
op=set  channels.discord.guilds.1465489501551067136.requireMention  true
```

This is brittle in three ways:

1. **Schema drift.** When OpenClaw updates its config schema, the driver silently breaks or injects unknown keys (which OpenClaw rejects at startup with `Unrecognized key`).
2. **Coverage gap.** Every new directive or surface scheme requires a driver update in Go. Novel runners need a bespoke driver from scratch.
3. **Complexity ceiling.** Rich surface configs (channel routing, per-guild policies, DM allowlists) require the driver to encode increasingly complex translation logic. Driver bloat is real — but the answer is not to strip features, it's to offload the translation.

---

## The Insight

The runner knows its own schema. The runner's own tools can configure it correctly. The runner's own validation can reject bad config. All of this is available if you're running inside (or alongside) the container.

An LLM worker running inside the runner image doesn't need to know the schema in advance — it can discover it, read the runner's documentation, try operations, validate the result with the runner's own tools, and self-correct. The driver's job shrinks from *translator* to *intent-generator + verifier*.

**The driver encodes WHAT. The worker figures out HOW.**

---

## Architecture

### Two Modes

**Build-time workers** — run after `claw build` as a post-build step:
- Spawned from the freshly built image
- Handle directives whose values are known at build time: `MODEL`, `HANDLE` platforms, `INVOKE` schedules
- Output a finalized config patch applied as a final image layer
- Result is baked into the image — `claw inspect` shows it, it's auditable, it's portable

**Deploy-time workers** — run at `claw up` before the actual container starts:
- Handle directives that require pod context: `SURFACE channel://discord` (needs runtime topology), `CONFIGURE` (needs env vars)
- Worker writes config files to a host-side staging directory
- Clawdapus mounts those files read-only into the actual container
- Matches the existing "write on host, mount `:ro`" enforcement model

Both modes share the same worker protocol. The distinction is when they run and what context they receive.

---

## Intent Manifest

The driver's first job is translating Clawfile directives and claw-pod.yml surfaces into a **runner-agnostic intent manifest**. This is what gets handed to the worker.

```yaml
# intent.yml — generated by claw from Clawfile + claw-pod.yml
runner: openclaw
runner_version: "2026.2.9"
mode: deploy-time          # build-time | deploy-time

intents:
  - id: model-primary
    type: model_pin
    description: "Set the primary model slot"
    slot: primary
    value: "anthropic/claude-sonnet-4-6"

  - id: heartbeat-schedule
    type: schedule
    description: "Heartbeat every 30 minutes"
    schedule: "*/30 * * * *"
    command: heartbeat

  - id: discord-channel
    type: enable_platform
    description: "Enable Discord channel integration with guild routing"
    platform: discord
    credential_env: DISCORD_TOKEN      # name only — worker never sees value
    routing:
      guilds:
        "1465489501551067136":
          policy: allowlist
          users: ["167037070349434880"]
          require_mention: true
      dm:
        enabled: true
        policy: allowlist
        allow_from: ["167037070349434880"]

  - id: discord-identity
    type: declare_identity
    description: "Declare public Discord user ID for pod-wide broadcast"
    platform: discord
    user_id: "${DISCORD_USER_ID}"     # resolved from env at deploy time

verification:
  - intent_id: model-primary
    check: "openclaw config get agents.defaults.model.primary"
    expected: "anthropic/claude-sonnet-4-6"
  - intent_id: discord-channel
    check: "openclaw config get channels.discord.enabled"
    expected: "true"
```

Intent types are runner-agnostic. The worker maps them to runner-native operations.

---

## Worker Context

When spawned, the worker container receives:

| Input | Source | Notes |
|-------|--------|-------|
| `intent.yml` | Generated by `claw` | Mounted at `/claw/intent.yml` |
| Runner docs | From image or mounted | The runner's own documentation, if available at a known path |
| Runner CLI | Already in image | The worker runs inside the runner image |
| Config file path | Known convention | e.g. `/workspace/openclaw.json` for OpenClaw |
| Env var names | From intent manifest | Worker knows which env vars to reference; does not receive their values at build time |

The worker does NOT receive:
- Secret values (only env var names — at deploy time, env vars are present in the container's environment)
- Network access beyond what's needed (isolated network)
- Write access beyond its staging config directory

The worker has access to:
- Runner CLI (`openclaw config set`, `openclaw config validate`, etc.)
- The runner's config files
- Runner documentation (if the image ships it)
- Shell, standard tools

---

## Worker Execution

The worker is a short-lived LLM agent. Its task is fixed:

> "Apply the intents in `/claw/intent.yml` using the tools available to you. For each intent, report what you did and provide a verification command and expected output."

The worker:
1. Reads the intent manifest
2. For each intent, determines the correct runner-native operation
3. Applies it (edits config, runs CLI, validates)
4. Self-corrects if the runner rejects the change
5. Emits a completion report

The worker is allowed to use the runner's own tooling for validation mid-flight — e.g. `openclaw config validate` after each write. This is more reliable than the driver guessing what valid config looks like.

---

## Completion Report

The worker emits a structured report on stdout:

```yaml
# completion-report.yml
status: success   # success | partial | failed
applied:
  - intent_id: model-primary
    status: applied
    action: "Set agents.defaults.model.primary = 'anthropic/claude-sonnet-4-6' in /workspace/openclaw.json"
    verification:
      cmd: "openclaw config get agents.defaults.model.primary"
      expected: "anthropic/claude-sonnet-4-6"

  - intent_id: discord-channel
    status: applied
    action: "Set channels.discord.enabled = true and routing config for guild 1465489501551067136"
    verification:
      cmd: "openclaw config get channels.discord.enabled"
      expected: "true"

  - intent_id: heartbeat-schedule
    status: failed
    reason: "Schema key 'agents.defaults.heartbeat.every' not found in current runner version"
    suggestion: "Try 'agents.heartbeat.interval' — found in openclaw config schema"
```

The `suggestion` field is the key advantage over hardcoded drivers: the worker can surface what it discovered, enabling operators and driver authors to update their intent mappings.

---

## Verification

Clawdapus reads the completion report and runs each `verification.cmd` inside a fresh read-only container:

```
verification_cmd → expected_output?
  YES → intent verified ✓
  NO  → verification failed → retry worker (up to N times) → fail-closed
```

Verification is independent of the worker — Clawdapus doesn't trust the worker's self-report alone. The verification commands are defined by the intent manifest (set by the driver), not by the worker.

This is the fail-closed guarantee: the container does not start unless every intent is independently verified.

---

## Driver Interface Evolution

### v1 (current) — Translator model

```go
type Driver interface {
    ApplyModel(slot, model string) error
    ApplySurface(surface ResolvedSurface) error
    ApplyConfigure(cmds []string) error
    Preflight() error
    PostApply() error
}
```

The driver encodes all translation logic. Complex configs require complex driver code.

### v2 — Intent-generator + verifier model

```go
type Driver interface {
    // Translate directives into runner-agnostic intent manifest
    GenerateIntents(directives []Directive, surfaces []ResolvedSurface) (IntentManifest, error)

    // Verify completion report — read-only checks against expected state
    Verify(report CompletionReport) error

    // Still needed: can the worker even run? (image present, config writable, etc.)
    Preflight() error

    // Declare what surface schemes this driver can generate intents for
    Capabilities() DriverCapabilities
}
```

The `GenerateIntents` method is much simpler than the current `Apply*` methods — it just describes what it wants, in a runner-agnostic vocabulary. The LLM worker handles the runner-specific implementation.

The `Verify` method reads back state and checks it — this is simpler too, because the completion report tells Clawdapus exactly what to check.

---

## Novel Runner Support

This is where the model pays off most.

For a runner without a bespoke driver:

1. Operator provides runner documentation at a known path in the image (e.g. `/claw/runner-docs.md`)
2. Clawdapus uses the **generic driver**, which generates broad, human-readable intents:
   ```yaml
   - type: generic_configure
     description: "Set primary LLM model to anthropic/claude-sonnet-4-6"
     value: "anthropic/claude-sonnet-4-6"
   ```
3. Worker receives intents + runner docs, figures out runner-specific implementation
4. Worker applies config using whatever tools the runner ships
5. Clawdapus verifies via generic checks (does the runner start? does health pass?)

Novel runners become governable without any driver development beyond documentation. The cost of onboarding a new runner drops dramatically.

---

## Worker Image

**Recommended: Same image as the runner.**

Rationale:
- The runner's own CLI is available (most reliable path to correct config)
- The runner can self-validate (`openclaw config validate`)
- No separate image to maintain or version-pin
- The runner's documentation may already be in the image

The worker runs as a separate short-lived container from the same image, with a different entrypoint:
```yaml
# injected by claw up, not in claw-pod.yml
<service>-worker:
  image: <same as service>
  entrypoint: ["claw-worker"]        # lightweight agent binary
  volumes:
    - ./intent.yml:/claw/intent.yml:ro
    - ./config-staging:/claw/config:rw
  network_mode: none                  # isolated
  restart: "no"
```

`claw-worker` is a small Go binary (or can be the `claw` binary itself in worker mode) that:
1. Spawns an LLM agent with the intent manifest as its task
2. Gives it tool access: shell, file read/write in `/claw/config`, runner CLI
3. Collects stdout as the completion report
4. Exits

---

## LLM Selection

The worker LLM is not the Claw's operational model. It runs for seconds during setup and is not part of the runtime. Appropriate selection:

- **Default:** a capable but cheap model (haiku-class) — this is a structured task, not creative reasoning
- **Configurable per driver or per deployment:** operators with unusual runners may want a stronger model for novel translation tasks
- **Declared in `CLAW_TYPE` or driver metadata**, not in Clawfile (operators shouldn't need to think about it)

The worker LLM call goes directly to the provider (not through cllama — setup happens before cllama is wired in).

---

## Failure Modes

| Failure | Response |
|---------|---------|
| Worker times out | Fail-closed — container does not start |
| Worker reports `failed` intent | Fail-closed with error message + suggestion from report |
| Verification fails after `success` report | Retry worker (max 3 attempts), then fail-closed |
| Runner rejects worker's config on startup | Detected via healthcheck failure — escalate to operator |
| LLM provider unavailable at deploy time | Fail-closed — cannot verify enforcement |

The last failure mode is a real operational concern: deploy-time workers require LLM provider availability. Mitigations:
- Build-time workers (bake config into image) eliminate deploy-time LLM dependency for static config
- Cache worker outputs — if nothing changed, reuse the last verified config
- Allow a "dry run" flag that skips worker for fast iteration (dev only, not production)

---

## Phase Placement

### Phase 2 (current) — v1 driver, hardcoded ops
Proves the driver framework with explicit Go-coded config mutations. Ship this first — it works, it's auditable, it doesn't depend on LLM availability.

### Phase 3 Slice 3 — Channel surface config is the first use case for workers
Channel surfaces are the first directive complex enough to justify the worker approach. The guild routing config is too rich for a simple hardcoded driver without significant ongoing maintenance burden. Channel surfaces become the first feature implemented worker-first.

The pattern proven here:
- OpenClaw driver generates channel surface intents
- Worker applies Discord/Slack/Telegram routing config using OpenClaw's CLI
- Clawdapus verifies `channels.<platform>.enabled = true` + spot-checks routing keys

### Phase 3.5 — HANDLE + social topology uses build-time workers
`HANDLE discord` → build-time worker writes `channels.discord.enabled = true` into the image. Simple enough to do worker-first without the v1 → v2 migration.

### Phase X — Full driver interface migration (v1 → v2)
After channel surfaces prove the pattern, migrate existing directives (`MODEL`, `INVOKE`, `CONFIGURE`) to the intent-generator model. v1 driver code is replaced by:
- A simple intent generator (a Go function that produces structured YAML)
- A simple verifier (a Go function that reads config keys)
- The LLM worker handles everything in between

---

## Relationship to Existing Concepts

| Concept | Relationship |
|---------|-------------|
| `CONFIGURE` directive | Worker-based `CONFIGURE` is a smarter version — LLM interprets the command in context rather than running it blindly |
| `ACT` mode | Worker mode is automated ACT — the LLM does what a human operator would do in worker mode |
| cllama sidecar | Complementary. Worker runs at setup; cllama runs at runtime. Different LLMs, different jobs. |
| Driver preflight | Unchanged — preflight checks whether the worker can even run, before spawning it |
| Fail-closed invariant | Strengthened — verification is now explicit and independently run |

---

## Open Questions

1. **Caching.** If the Clawfile hasn't changed, can we skip the worker and reuse the last verified config? How do we detect "nothing changed" reliably?

2. **Build-time worker LLM keys.** Build-time workers need LLM access. Where do provider keys come from during `claw build`? From the operator's environment (`ANTHROPIC_API_KEY`), same as any other build tool.

3. **Worker vs CONFIGURE ordering.** `CONFIGURE` directives run at container init. Workers run before the container starts. The ordering contract: workers run first (writing config files), CONFIGURE runs second (mutations against those files). This needs to be explicit in the entrypoint wrapper.

4. **Partial failure.** If 3 of 5 intents are applied and the 4th fails, should the container start with partial configuration? No — fail-closed. But the completion report should name exactly which intents failed and why, so operators can debug quickly.

5. **Worker versioning.** The worker LLM may behave differently across model versions. Should worker outputs be reproducible? Consider pinning the worker model version per driver release, same as any other dependency.
