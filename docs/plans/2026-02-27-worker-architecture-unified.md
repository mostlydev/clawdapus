# Unified Worker Architecture

**Date:** 2026-02-27
**Status:** Design — not yet implemented
**Supersedes:** `docs/plans/2026-02-21-llm-configuration-workers.md` (LLM Configuration Workers), `docs/brainstorming/init-claw-service-engineer.md` (Init-Claws)
**Related:** ADR-002 (Runtime Authority), ADR-003 (Config Injection Model)

---

## Summary

Two proposals describe short-lived LLM workers that run before (or after) the main claw container:

1. **LLM Configuration Workers** — translate runner-agnostic intents into runner-native config using the runner's own CLI and docs.
2. **Init-Claws (Service Engineers)** — assess, provision, and validate the host environment; produce human-readable diagnostics on failure.

These are the same infrastructure with different jobs. This plan reconciles them into a single worker framework with multiple **worker roles**.

---

## Worker Roles

A worker is a short-lived container spawned from the same image as the target claw, with a scoped task, structured I/O, and a deadline. Workers differ by **role**, which determines their inputs, permitted actions, and when they run.

### Role 1: Config Worker (from LLM Configuration Workers plan)

**When:** Pre-start (build-time or deploy-time)
**Job:** Translate intent manifest → runner-native config using runner CLI + docs.
**Inputs:** `intent.yml` (generated by driver), runner docs, runner CLI.
**Outputs:** Config files in staging dir + `completion-report.yml`.
**Write access:** Config staging directory only.
**Verification:** Clawdapus runs verification commands from intent manifest independently.

This is the core of the original workers plan. No changes to the intent manifest, completion report, or verification protocol described there.

### Role 2: Provision Worker (from Init-Claws — "Groundwork")

**When:** Pre-start, runs *before* the config worker.
**Job:** Prepare the environment — scaffold directories, check host dependencies, validate volume mount structure, download required stubs.
**Inputs:** `provision-manifest.yml` (generated by driver or declared in pod YAML).
**Outputs:** Prepared volumes + `provision-report.yml`.
**Write access:** Target claw's writable volumes (config dir, state dir, cron dir). NOT the host filesystem outside declared volumes.
**Verification:** Clawdapus checks that expected paths exist and are non-empty.

This is the "Groundwork Phase" from the init-claw doc, scoped properly.

### Role 3: Diagnostic Worker (new — synthesized from Init-Claws)

**When:** Post-failure. Triggered when a claw exits non-zero or fails healthcheck N times.
**Job:** Read container logs, inspect config state, produce a structured diagnostic explaining what went wrong and suggesting fixes.
**Inputs:** Container logs (from Docker SDK), config files, healthcheck output.
**Outputs:** `diagnostic-report.yml` — structured error analysis with severity, root cause hypothesis, and suggested fix.
**Write access:** None. Read-only.
**Verification:** N/A — diagnostic workers don't mutate state.

This is the highest-value idea from the init-claw doc. Crash-loop debugging is where operators lose hours. A worker that reads the logs and says "your Discord token is expired, here's the error from line 47" is worth more than self-healing.

---

## Execution Order

```
claw up
  │
  ├─ Parse Clawfile + claw-pod.yml
  ├─ Driver preflight (existing — can the worker even run?)
  │
  ├─ [1] Provision Worker (if declared)
  │     └─ Scaffold dirs, check deps, prepare volumes
  │     └─ Fail-closed if provision-report.status != success
  │
  ├─ [2] Config Worker (if intents exist)
  │     └─ Translate intents → runner-native config
  │     └─ Verification: independent check per intent
  │     └─ Fail-closed if any intent unverified
  │
  ├─ Emit compose.generated.yml
  ├─ docker compose up
  │
  └─ [3] Diagnostic Worker (on failure only)
        └─ Read logs + config, emit diagnostic-report.yml
        └─ Report surfaced via claw health / claw inspect
```

Step 1 and 2 are strictly ordered (provision before config — config worker needs the directories to exist). Step 3 is reactive, not part of the startup sequence.

---

## Trust Boundary

Workers are **not untrusted workloads** in the same way long-running claws are. They are short-lived, single-task, operator-initiated processes. But they still need guardrails:

### What workers CAN do
- Read/write files within declared volume mounts (provision, config workers)
- Read container logs via mounted log files (diagnostic worker)
- Execute runner CLI commands (`openclaw config set`, `openclaw config validate`)
- Make LLM API calls (config + provision workers need this; diagnostic may too)

### What workers CANNOT do
- Write outside declared volume paths (enforced by container volume mounts)
- Access other claws' volumes or config
- Access the Docker socket
- Persist beyond their deadline (hard timeout, enforced by Clawdapus)
- Access secrets they don't need (credential names only in intent manifest, not values — except at deploy-time when env vars are naturally present)

### Enforcement
- Workers run in containers with the same `read_only: true` + `tmpfs` baseline as claws
- Volume mounts are explicitly scoped per role (config worker gets config staging dir; provision worker gets target volumes; diagnostic worker gets read-only log mount)
- Hard deadline: configurable, default 60s for config/provision, 30s for diagnostic
- Network: `network_mode: none` for config workers (no egress needed beyond LLM call, which is handled differently — see LLM Selection). Provision workers that need to download stubs get scoped egress

---

## Manifest Formats

### Provision Manifest

```yaml
# provision-manifest.yml — generated by driver or from x-claw block
role: provision
checks:
  - id: config-dir-exists
    type: directory_exists
    path: /app/config
    create_if_missing: true

  - id: state-dir-exists
    type: directory_exists
    path: /app/state
    create_if_missing: true

  - id: cron-dir-writable
    type: directory_writable
    path: /app/state/cron

  - id: node-version
    type: command_check
    cmd: "node --version"
    constraint: ">=18.0.0"
    severity: error     # error = fail-closed, warning = log and continue

  # Open-ended tasks that benefit from LLM reasoning
  - id: install-updates
    type: freeform
    description: "Check if the runner has pending updates and install them if available"
    timeout: 30s

  - id: validate-skill-deps
    type: freeform
    description: "Verify that all skill files referenced in /app/skills/ have their dependencies satisfied"
    timeout: 15s
```

The `freeform` type is where the LLM earns its keep — tasks that are hard to express as a deterministic check but easy to describe in natural language. The LLM runs inside the runner image where all the tools are available.

### Diagnostic Report

```yaml
# diagnostic-report.yml — emitted by diagnostic worker
status: diagnosed    # diagnosed | inconclusive
claw: percy
exit_code: 1
analysis:
  - severity: error
    component: discord-gateway
    summary: "Discord bot token rejected — received 401 Unauthorized"
    evidence: "Log line 47: 'Error: An invalid token was provided.'"
    suggestion: "Verify DISCORD_TOKEN env var is set and the bot has not been reset in Discord Developer Portal"

  - severity: warning
    component: config
    summary: "Config key 'agents.list[0].groupChat.mentionPatterns' is empty"
    evidence: "Config file /app/config/openclaw.json line 23"
    suggestion: "HANDLE directive may not have been applied — check claw health"
```

### Intent Manifest + Completion Report

Unchanged from the LLM Configuration Workers plan. See that document for the full schema.

---

## Declaration

Workers are NOT declared via a `Clawfile` directive. The Clawfile is a build-time artifact that transpiles to Dockerfile instructions; worker orchestration is a runtime concern.

Workers are declared in one of two places:

### 1. Driver metadata (automatic)

The driver decides which worker roles are needed based on the directives it processes. If a driver generates intents, a config worker runs automatically. If the driver detects volume requirements, it can emit a provision manifest.

```go
type Driver interface {
    // ... existing methods ...

    // New: declare what workers this driver needs
    WorkerRequirements(directives []Directive, surfaces []ResolvedSurface) []WorkerRequirement
}
```

### 2. Pod YAML (operator override)

Operators can explicitly request workers or configure their behavior:

```yaml
# claw-pod.yml
services:
  percy:
    x-claw:
      type: openclaw
      workers:
        provision:
          enabled: true
          timeout: 90s
        config:
          model: anthropic/claude-haiku-4-5  # override default worker LLM
        diagnostic:
          enabled: true                      # opt-in: run on crash
          on_failure: report                 # report | retry-then-report
```

This aligns with existing `x-claw` patterns (`cllama`, `surfaces`, `handles`) and avoids the `INIT_AGENT` Clawfile directive problem Codex identified.

---

## LLM Selection

Worker LLM is not the claw's operational model. It runs for seconds and performs structured tasks.

- **Default:** Haiku-class model. Structured config translation doesn't need frontier reasoning.
- **Configurable:** Per-worker-role in pod YAML (see above) or per-driver in driver metadata.
- **Provider:** Direct API call to operator's configured provider. NOT through cllama — workers run before cllama is wired.
- **Credentials:** Operator's `ANTHROPIC_API_KEY` (or equivalent) from host environment, injected as env var into worker container.

For **diagnostic workers**, a stronger model may be warranted — log analysis and root-cause reasoning benefit from more capable models. Operators can override per-role.

---

## Audit Trail

Every worker run produces artifacts that are retained and inspectable:

| Artifact | Location | Inspectable via |
|----------|----------|----------------|
| Provision manifest | `.claw/workers/<service>/provision-manifest.yml` | `claw inspect <service>` |
| Provision report | `.claw/workers/<service>/provision-report.yml` | `claw inspect <service>` |
| Intent manifest | `.claw/workers/<service>/intent.yml` | `claw inspect <service>` |
| Completion report | `.claw/workers/<service>/completion-report.yml` | `claw inspect <service>` |
| Diagnostic report | `.claw/workers/<service>/diagnostic-report.yml` | `claw health` |
| Volume diff | `.claw/workers/<service>/volume-diff.txt` | `claw inspect <service>` |

The **volume diff** captures what the provision worker changed — snapshot before, diff after. This preserves inspectability for open-ended `freeform` tasks. If the worker ran `apt update` or modified 3 files, the diff shows exactly what.

---

## Idempotency

Workers MUST be idempotent. Running `claw up` twice should produce the same result.

- **Config workers:** Write to a clean staging directory each time. Old staging dir is wiped before the worker runs. Config is always generated fresh from intents.
- **Provision workers:** `directory_exists` + `create_if_missing` is naturally idempotent. `freeform` tasks are instructed to check before acting ("install updates only if not already at latest"). The volume diff audit trail lets operators verify.
- **Diagnostic workers:** Read-only. Idempotent by definition.

---

## Failure Modes

| Failure | Response |
|---------|---------|
| Provision worker times out | Fail-closed — claw does not start |
| Provision check fails (severity: error) | Fail-closed with structured error from provision report |
| Provision check fails (severity: warning) | Log warning, continue |
| Config worker times out | Fail-closed |
| Config worker reports failed intent | Fail-closed with error + suggestion from completion report |
| Config verification fails after success report | Retry worker (max 3), then fail-closed |
| Diagnostic worker times out | Log timeout, do not block (diagnostic is advisory) |
| LLM provider unavailable | Fail-closed for config/provision workers. Diagnostic worker degrades to log tail |
| Worker exits non-zero | Fail-closed (config/provision). Log error (diagnostic) |

---

## Relationship to Existing Systems

| Existing Concept | Relationship |
|-----------------|-------------|
| Driver preflight | Unchanged. Runs before workers — checks whether workers can even run |
| Config injection (current) | Workers eventually replace direct Go-coded config patching. Migration is gradual: current v1 driver works alongside workers during transition |
| `CONFIGURE` directive | Worker-based CONFIGURE is smarter — LLM interprets in context rather than blind execution |
| cllama sidecar | Complementary. Workers run at setup; cllama runs at runtime. Different LLMs, different jobs |
| `claw health` | Extended to surface diagnostic reports alongside existing healthcheck output |
| `inspectImageEnv` | Unchanged. Still runs before workers to catch baked credentials |
| Fail-closed invariant | Strengthened. Verification is now explicit and independently run per intent |

---

## Phase Placement

This is not yet assigned to a phase in the implementation plan. Candidate placement:

| Phase | Worker Role | Rationale |
|-------|-------------|-----------|
| Phase 5 or 6 | Config worker (deploy-time) | Channel surface config is complex enough to justify the worker approach. Proves the intent→worker→verify pipeline |
| Phase 5 or 6 | Provision worker | Natural companion to config workers — runs first in the sequence |
| Phase 5 or 6 | Diagnostic worker | High operator value, low implementation risk (read-only), can ship independently |
| Later | Config worker (build-time) | Baking config into images requires changes to `claw build`. Deferred until deploy-time is proven |
| Later | Full v1 → v2 driver migration | After workers prove reliable, migrate existing hardcoded driver ops to intent-generator model |

The diagnostic worker could arguably ship first — it's the simplest (read-only, no verification contract, no intent protocol) and provides immediate operator value. It doesn't require the full intent manifest infrastructure.

---

## Open Questions (carried forward + new)

1. **Caching.** Can we skip workers when nothing changed? Hash of (Clawfile + pod YAML + image digest) as cache key. If hash matches last verified run, reuse config. (From workers plan)

2. **Build-time worker LLM keys.** Build-time workers need LLM access during `claw build`. Source: operator's host environment (`ANTHROPIC_API_KEY`). (From workers plan)

3. **Worker vs CONFIGURE ordering.** Workers run first (write config files), CONFIGURE runs second (mutations against those files). Explicit in entrypoint wrapper. (From workers plan)

4. **Freeform task guardrails.** How much latitude does a `freeform` provision task get? "Install updates" could mean `apt upgrade` which could break the image. Mitigation: freeform tasks run in a throwaway container layer; if verification fails, the layer is discarded. (New)

5. **Diagnostic worker trigger.** Exact conditions: exit code != 0? Healthcheck failure count threshold? Configurable per-claw? Default should be conservative — don't spin up an LLM worker on every transient restart. (New)

6. **Multi-claw ordering.** In a pod with claws A and B, do provision workers run in parallel or sequentially? Config workers? If A's config depends on B's handle (peer topology), B's config worker must complete first. Dependency graph needed. (New — from init-claw review)
